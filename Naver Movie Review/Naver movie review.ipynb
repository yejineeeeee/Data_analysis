{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\82108\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\82108\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\82108\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\82108\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from konlpy) (4.6.1)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from konlpy) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from konlpy) (1.22.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\82108\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (20.4)\n",
      "Requirement already satisfied: six in c:\\users\\82108\\anaconda3\\lib\\site-packages (from packaging->JPype1>=0.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from packaging->JPype1>=0.7.0->konlpy) (2.4.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup \n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = []\n",
    "for i in range(1, 500):\n",
    "    url = f\"https://movie.naver.com/movie/point/af/list.naver?&page={i}\"\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.content,'html.parser')\n",
    "    reviews = soup.find_all(\"td\",{\"class\":\"title\"})\n",
    "\n",
    "    #한 페이지의 리뷰 리스트의 리뷰를 하나씩 보면서 데이터 추출\n",
    "    for review in reviews:\n",
    "        sentence = review.find(\"a\",{\"class\":\"report\"}).get(\"onclick\").split(\"', '\")[2]\n",
    "        movie = review.find(\"a\",{\"class\":\"movie color_b\"}).get_text()\n",
    "        if movie.find('슬램덩크') != -1 and sentence != \"\":\n",
    "#             print(movie)\n",
    "            score = review.find(\"em\").get_text()\n",
    "            review_data.append([movie,sentence,int(score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['더 퍼스트 슬램덩크', '만화에 비하면 아쉽지만 그래도 조아따요~ ', 9], ['더 퍼스트 슬램덩크', '최고의 슬램덩크 영화 추천', 10], ['더 퍼스트 슬램덩크', '추억속으로 확 빨려드는 느낌이랄까.. 영화보고 훌쩍이고 집에와서 몇십년전에 사둔 만화책을 주섬주섬 꺼내 다시 읽었습니다.너무 좋아요', 10], ['더 퍼스트 슬램덩크', '마지막 1분은 정말 역대급.. 최고!', 10], ['더 퍼스트 슬램덩크', '은근히 빠져드는 스토리보기싫어했는데 내가 제일 열심히봤다ㅋㅋ', 10], ['더 퍼스트 슬램덩크', '한....5번 본거 같아요 만화 안보고 세번정도 만화보고 두번정도 봤는데 볼수록 &amp;#39;아 000도 잠깐 나오는 구나!&amp;#39; 하며 찾는 재미도 있고요 만화 속 인물들과 괴리감이 별로 안느껴져요 그리고 처음 볼때는 농구 영화다 보니까 스토리에만 집중 했었는데 농구에 대해 좀 더 배우고 룰 등을 알게되니 어떤 선수가 어떻게 행동하는지 보여서 재밌었어요 흠.. 혹시 보러갈 예정이시면 짤막하게 스토리 요약을 보고 가시면 좀 더 재밌고요 농구 포지션이나 스킬도 알아두시면 더더욱 재밌으실거예요', 10], ['더 퍼스트 슬램덩크', '오프닝부터 울리는 소리는 베이스소리만은 아니었을거야.전체가 올타임 레전드...', 10], ['더 퍼스트 슬램덩크', '재미는 있습니다.보지는 않았지만 아바타나 유렁 기타 등등 다른 영화보다는 이 영화를 추천해요그런데 지금 더 퍼스트 슬램덩크의 평점을 보면 솔직히 이 정도가 싶습니다.이 영화가 반지의 제왕이나 라이언 일병 구하기 센과 치히로보다 더 감동이나 재미가 있었다라고 하면 절대 아니거든요아무리 높게 쳐봐야 8점 후반때인데9점 중반은... 뭐 그렇습니다', 8], ['더 퍼스트 슬램덩크', '슬램덩크 모르고 봤는데..그냥 미쳤네요 이게 말로 표현이 안돼요..숨참으면서 봤어요ㅠ', 10], ['더 퍼스트 슬램덩크', '진짜 재밌네요 슬램덩크를 안봤는데도', 10]]\n"
     ]
    }
   ],
   "source": [
    "print(review_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = [\"movie\",\"sentence\",\"score\"]\n",
    "with open ( \"naver_review.csv\", \"w\", newline =\"\",encoding = 'utf8' ) as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(columns_name)\n",
    "    write.writerows(review_data)\n",
    "review_df = pd.read_csv(\"naver_review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>더 퍼스트 슬램덩크</td>\n",
       "      <td>만화에 비하면 아쉽지만 그래도 조아따요~</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>더 퍼스트 슬램덩크</td>\n",
       "      <td>최고의 슬램덩크 영화 추천</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>더 퍼스트 슬램덩크</td>\n",
       "      <td>추억속으로 확 빨려드는 느낌이랄까.. 영화보고 훌쩍이고 집에와서 몇십년전에 사둔 만...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>더 퍼스트 슬램덩크</td>\n",
       "      <td>마지막 1분은 정말 역대급.. 최고!</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>더 퍼스트 슬램덩크</td>\n",
       "      <td>은근히 빠져드는 스토리보기싫어했는데 내가 제일 열심히봤다ㅋㅋ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>더 퍼스트 슬램덩크</td>\n",
       "      <td>10점…10점이요!</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>더 퍼스트 슬램덩크</td>\n",
       "      <td>진짜 소리 진짜 너무 좋습니다.더빙을 강력 추천...몸짓 하나하나 진짜 너무 디테일...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>더 퍼스트 슬램덩크</td>\n",
       "      <td>영화관에서 안보면 두고두고 후회할 영화</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>더 퍼스트 슬램덩크</td>\n",
       "      <td>오프닝부터 엔딩까지 소름 반가움 감동</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>더 퍼스트 슬램덩크</td>\n",
       "      <td>몇 번을 더 봐야 경기장면  말미에서 편안하게 볼 수 있는겁니까</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie                                           sentence  score\n",
       "0    더 퍼스트 슬램덩크                            만화에 비하면 아쉽지만 그래도 조아따요~       9\n",
       "1    더 퍼스트 슬램덩크                                     최고의 슬램덩크 영화 추천     10\n",
       "2    더 퍼스트 슬램덩크  추억속으로 확 빨려드는 느낌이랄까.. 영화보고 훌쩍이고 집에와서 몇십년전에 사둔 만...     10\n",
       "3    더 퍼스트 슬램덩크                               마지막 1분은 정말 역대급.. 최고!     10\n",
       "4    더 퍼스트 슬램덩크                  은근히 빠져드는 스토리보기싫어했는데 내가 제일 열심히봤다ㅋㅋ     10\n",
       "..          ...                                                ...    ...\n",
       "267  더 퍼스트 슬램덩크                                         10점…10점이요!     10\n",
       "268  더 퍼스트 슬램덩크  진짜 소리 진짜 너무 좋습니다.더빙을 강력 추천...몸짓 하나하나 진짜 너무 디테일...     10\n",
       "269  더 퍼스트 슬램덩크                              영화관에서 안보면 두고두고 후회할 영화     10\n",
       "270  더 퍼스트 슬램덩크                               오프닝부터 엔딩까지 소름 반가움 감동     10\n",
       "271  더 퍼스트 슬램덩크                몇 번을 더 봐야 경기장면  말미에서 편안하게 볼 수 있는겁니까     10\n",
       "\n",
       "[272 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'더 퍼스트 슬램덩크'의 평균 평점은 9.35\n"
     ]
    }
   ],
   "source": [
    "print(\"'더 퍼스트 슬램덩크'의 평균 평점은 {:.2f}\".format(review_df['score'].mean()))\n",
    "#'타이타닉'의 평균 평점은 9.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'sudo'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'rm'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install -y fonts-nanum\n",
    "!sudo fc-cache -fv\n",
    "!rm ~/.cache/matplotlib -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<konlpy.tag._okt.Okt object at 0x000001F0713728E0>\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 경고문구 미표시\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 지정\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "\n",
    "# 트위터 형태소 분석기(Okt)를 활용\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "print(okt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다운로드 받을 폴더를 준비\n",
    "DATA_DIR = \"./data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-bcd4d35c389c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m torchtext.utils.download_from_url(url='https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt', \n\u001b[0;32m      3\u001b[0m                                   path=os.path.join(DATA_DIR, 'review.txt'))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import torchtext \n",
    "torchtext.utils.download_from_url(url='https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt', \n",
    "                                  path=os.path.join(DATA_DIR, 'review.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/data/review.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8ca7ccfdcb3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# txt 파일을 판다스 데이터프레임으로 읽어오기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/data/review.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 데이터프레임의 크기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/review.txt'"
     ]
    }
   ],
   "source": [
    "# txt 파일을 판다스 데이터프레임으로 읽어오기\n",
    "data = pd.read_csv('/content/data/review.txt', sep='\\t')\n",
    "\n",
    "# 데이터프레임의 크기\n",
    "print(data.shape)\n",
    "\n",
    "# 첫 5행을 출력\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-6208d269f320>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\82108\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from konlpy) (4.6.1)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from konlpy) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from konlpy) (1.22.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\82108\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (20.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from packaging->JPype1>=0.7.0->konlpy) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\82108\\anaconda3\\lib\\site-packages (from packaging->JPype1>=0.7.0->konlpy) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# 코랩에 konlpy 패키지를 설치합니다.\n",
    "! pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 토큰화 및 불용어 제거\n",
    "okt = Okt()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(data, num_words=10000):\n",
    "    #데이터의 중복을 제거\n",
    "    data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    #데이터의 결측치를 제거\n",
    "    data = data.dropna(how = 'any') \n",
    "    data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    data = data.dropna(how = 'any') \n",
    "    \n",
    "    X_train = []\n",
    "    for sentence in data['document']:\n",
    "        #데이터를 토큰화 시킨다 ex) 안녕 = 1\n",
    "        tokenized = okt.morphs(sentence, stem = True) # 토큰화\n",
    "        #stopwords에 있는 불용어들이 문장에 있으면 제거 해줍니다.\n",
    "        stopwords_removed_sentence = [word for word in tokenized if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(stopwords_removed_sentence)\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f3623b9c3ca1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = load_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a049ca3b19ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-dbd883db58b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1bfbd4ce064b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 누락 데이터를 제거\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreview_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'document'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 학습 속도를 고려하여, 1000개의 샘플을 선택하여 추출\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mreview_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreview_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# 누락 데이터를 제거\n",
    "review_data = data['document'].dropna().values\n",
    "\n",
    "# 학습 속도를 고려하여, 1000개의 샘플을 선택하여 추출\n",
    "review_data = review_data[:1000]\n",
    "\n",
    "# 배열의 크기\n",
    "print(review_data.shape)\n",
    "\n",
    "# 첫 번째 데이터\n",
    "print(review_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/272 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "phrase input should be string, not <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d3aa7f4adc92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mokt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcleaned_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py\u001b[0m in \u001b[0;36mnouns\u001b[1;34m(self, phrase)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;34m\"\"\"Noun extractor.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mtagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Noun'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mjoined\u001b[0m \u001b[0msets\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmorph\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \"\"\"\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mvalidate_phrase_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         tokens = self.jki.tokenize(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_common.py\u001b[0m in \u001b[0;36mvalidate_phrase_inputs\u001b[1;34m(phrase)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \"\"\"\n\u001b[0;32m     19\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"phrase input should be string, not %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: phrase input should be string, not <class 'list'>"
     ]
    }
   ],
   "source": [
    "# 세 글자 이상의 명사를 사용 (두 글자 이하의 단어는 제거)\n",
    "cleaned_review_data = []\n",
    "\n",
    "for review in tqdm(review_data):\n",
    "    tokens = okt.nouns(review)\n",
    "    cleaned_tokens = []\n",
    "    for word in tokens:\n",
    "        if len(word) > 2:\n",
    "            cleaned_tokens.append(word)\n",
    "        else:\n",
    "            pass\n",
    "    cleaned_review = \" \".join(cleaned_tokens)\n",
    "    cleaned_review_data.append(cleaned_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_review_data))\n",
    "print(cleaned_review_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF 벡터로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ec506889ef8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# TF-IDF 변환기에 데이터를 입력하여 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mreview_tfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# 배열의 크기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \"\"\"\n\u001b[0;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 사이킷런 패키지 활용 \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF 변환기 객체를 생성\n",
    "tfid = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF 변환기에 데이터를 입력하여 변환\n",
    "review_tfid = tfid.fit_transform(review_data)\n",
    "\n",
    "# 배열의 크기\n",
    "print(review_tfid.shape)\n",
    "\n",
    "# 첫 번째 데이터\n",
    "print(review_tfid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'vocabulary_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-acb0c5424f73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 단어 사전 확인 (딕셔너리 형태)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 단어 사전의 크기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'vocabulary_'"
     ]
    }
   ],
   "source": [
    "# 단어 사전 확인 (딕셔너리 형태)\n",
    "vocab = tfid.vocabulary_\n",
    "\n",
    "# 단어 사전의 크기\n",
    "print(len(vocab))\n",
    "\n",
    "# 단어 사전 출력 (앞에서 5개의 단어만 출력)\n",
    "print({ k:v for i, (k, v) in enumerate(vocab.items()) if i < 15 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-e8eaa37df731>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 단어들의 사전 인덱스를 이용하여 원래 단어를 검색하는 매핑 딕셔너리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mindex_to_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 앞에서 5개의 단어를 출력\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m  \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_to_word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m15\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# 단어들의 사전 인덱스를 이용하여 원래 단어를 검색하는 매핑 딕셔너리\n",
    "index_to_word = { v:k for k, v in vocab.items() } \n",
    "\n",
    "# 앞에서 5개의 단어를 출력\n",
    "print({  k:v for i, (k, v) in enumerate(index_to_word.items()) if i < 15 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'review_tfid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-2eeeda25ed69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# TF-IDF 벡터를 입력하여 모델 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview_tfid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'review_tfid' is not defined"
     ]
    }
   ],
   "source": [
    "# 사이킷런 패키지 활용\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# LDA 모델링 객체를 생성 (토픽 갯수를 2로 설정: 긍정/부정)\n",
    "lda = LatentDirichletAllocation(n_components=2)  \n",
    "\n",
    "# TF-IDF 벡터를 입력하여 모델 학습 \n",
    "lda.fit(review_tfid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LatentDirichletAllocation' object has no attribute 'components_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-75015075db4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 토픽 모델링 결과를 담고 있는 배열의 형태 : (2개의 토픽, 2157개의 단어)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'LatentDirichletAllocation' object has no attribute 'components_'"
     ]
    }
   ],
   "source": [
    "# 토픽 모델링 결과를 담고 있는 배열의 형태 : (2개의 토픽, 2157개의 단어)\n",
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-32-41dec9559c1c>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-32-41dec9559c1c>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    print(f\"토픽 유형 {idx+1}:\", [(index_to_word[i], topic[i].round(3)) for i in topic.argsort\u001b[0m\n\u001b[1;37m                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# 2157개의 단어 중에서, 토픽 별로 가장 중요도가 높은 단어를 10개씩 출력\n",
    "\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"토픽 유형 {idx+1}:\", [(index_to_word[i], topic[i].round(3)) for i in topic.argsort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyLDAvis 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "gensim 4.3.0 requires scipy>=1.7.0, but you'll have scipy 1.5.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading pyLDAvis-3.4.0-py2.py3-none-any.whl (2.6 MB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.18-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: numexpr in c:\\users\\82108\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.7.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\82108\\anaconda3\\lib\\site-packages (from pyLDAvis) (50.3.1.post20201107)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.22.3)\n",
      "Collecting pandas>=1.3.4\n",
      "  Downloading pandas-1.5.3-cp38-cp38-win_amd64.whl (11.0 MB)\n",
      "Collecting scikit-learn>=1.0.0\n",
      "  Downloading scikit_learn-1.2.1-cp38-cp38-win_amd64.whl (8.3 MB)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.0-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\82108\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.1.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Collecting Cython==0.29.32\n",
      "  Downloading Cython-0.29.32-py2.py3-none-any.whl (986 kB)\n",
      "Collecting FuzzyTM>=0.4.0\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.3.4->pyLDAvis) (1.15.0)\n",
      "Collecting pyfume\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "Collecting simpful\n",
      "  Downloading simpful-2.9.0-py3-none-any.whl (30 kB)\n",
      "Collecting fst-pso\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\82108\\anaconda3\\lib\\site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (2.24.0)\n",
      "Collecting miniful\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (2.10)\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py): started\n",
      "  Building wheel for fst-pso (setup.py): finished with status 'done'\n",
      "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20445 sha256=5109c5dab8b5a1e0c5f11325f097f94065a9e025d6610457b5e147a636603d0e\n",
      "  Stored in directory: c:\\users\\82108\\appdata\\local\\pip\\cache\\wheels\\6a\\65\\c4\\d27eeee9ba3fc150a0dae150519591103b9e0dbffde3ae77dc\n",
      "  Building wheel for miniful (setup.py): started\n",
      "  Building wheel for miniful (setup.py): finished with status 'done'\n",
      "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3534 sha256=466d2cf46a1356e7f41bf47db391fc726ea44b88561cc73760d6374fd7158337\n",
      "  Stored in directory: c:\\users\\82108\\appdata\\local\\pip\\cache\\wheels\\ba\\d9\\a0\\ddd93af16d5855dd9bad417623e70948fdac119d1d34fb17c8\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: joblib, funcy, pandas, scikit-learn, smart-open, Cython, simpful, miniful, fst-pso, pyfume, FuzzyTM, gensim, pyLDAvis\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 0.17.0\n",
      "    Uninstalling joblib-0.17.0:\n",
      "      Successfully uninstalled joblib-0.17.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.3\n",
      "    Uninstalling pandas-1.1.3:\n",
      "      Successfully uninstalled pandas-1.1.3\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.2\n",
      "    Uninstalling scikit-learn-0.23.2:\n",
      "      Successfully uninstalled scikit-learn-0.23.2\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.21\n",
      "    Uninstalling Cython-0.29.21:\n",
      "      Successfully uninstalled Cython-0.29.21\n",
      "Successfully installed Cython-0.29.32 FuzzyTM-2.0.5 fst-pso-1.8.1 funcy-1.18 gensim-4.3.0 joblib-1.2.0 miniful-0.0.6 pandas-1.5.3 pyLDAvis-3.4.0 pyfume-0.2.25 scikit-learn-1.2.1 simpful-2.9.0 smart-open-6.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ClassNamePrefixFeaturesOutMixin' from 'sklearn.base' (C:\\Users\\82108\\anaconda3\\lib\\site-packages\\sklearn\\base.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-4692c0702709>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# LDA 토픽 모델링 결과를 시각화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvisualization\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview_tfid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyLDAvis\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"3.4.0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjs_PCoA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPreparedData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyLDAvis\\_display.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_server\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mserve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_ipynb_local_js\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumPyEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPreparedData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murls\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquareform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMDS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNumPyEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_locally_linear\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlocally_linear_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLocallyLinearEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_isomap\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIsomap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_mds\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMDS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmacof\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_locally_linear.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meigsh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m from ..base import (\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mBaseEstimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mTransformerMixin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ClassNamePrefixFeaturesOutMixin' from 'sklearn.base' (C:\\Users\\82108\\anaconda3\\lib\\site-packages\\sklearn\\base.py)"
     ]
    }
   ],
   "source": [
    "# LDA 토픽 모델링 결과를 시각화\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "visualization = pyLDAvis.sklearn.prepare(lda, review_tfid, tfid)\n",
    "pyLDAvis.display(visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental Analysis (감정 분석)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 감정 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['label'].iloc[:1000].values\n",
    "print(labels.shape)\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런 패키지 활용\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 로지스틱 분류 모델링 객체를 생성 \n",
    "lr = LogisticRegression()\n",
    "\n",
    "# TF-IDF 벡터를 입력하여 모델 학습 \n",
    "lr.fit(review_tfid, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 샘플을 이용하여 모델 예측\n",
    "pred = lr.predict(review_tfid[0])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 판다스 데이터프레임로 읽어오기\n",
    "test = pd.read_csv('naver_review.csv')\n",
    "\n",
    "test.head()\n",
    "# print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_predict(i):\n",
    "  test_sample = test['sentence'][i]\n",
    "  # 한글을 제외하고 전부 제거\n",
    "  test_sample = re.sub(r\"[^가-힣]\", \"\", test_sample)\n",
    "  tokens = okt.nouns(test_sample)\n",
    "  cleaned_tokens = []\n",
    "  for word in tokens:\n",
    "      if len(word) > 1:\n",
    "          cleaned_tokens.append(word)\n",
    "      else:\n",
    "          pass\n",
    "  cleaned_review = \" \".join(cleaned_tokens)\n",
    "   # TF-IDF 변환기에 데이터를 입력하여 변환\n",
    "  test_review_tfid = tfid.transform([cleaned_review])\n",
    "  # # 배열을 출력\n",
    "  # print(test_review_tfid)\n",
    "  # 로지스틱 회귀 모델을 활용하여 분류 예측\n",
    "  test_pred = lr.predict(test_review_tfid)[0]\n",
    "  return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = 0 # 긍정 리뷰를 긍정으로 예측한 경우\n",
    "nn = 0 # 부정 리뷰를 부정으로 예측한 경우\n",
    "np = 0 # 부정 리뷰를 긍정으로 예측한 경우\n",
    "pn = 0 # 긍정 리뷰를 부정으로 예측한 경우\n",
    "for i in range(150):\n",
    "  print(test['sentence'][i])\n",
    "  test_pred = logistic_predict(i)\n",
    "  print(\"분석 결과 {}적인 리뷰로 예측됩니다. \".format(\"긍정\" if test_pred > 0 else \"부정\"))\n",
    "  print(test['score'][i])\n",
    "  if test_pred  > 0:\n",
    "    if test['score'][i] > 7: # 긍정리뷰를 10 ~ 7 로 가정\n",
    "      pp += 1\n",
    "    else:\n",
    "      np += 1\n",
    "  else:\n",
    "    if test['score'][i] > 7: # 긍정리뷰를 10 ~ 7 로 가정\n",
    "      pn += 1\n",
    "    else:\n",
    "      nn += 1\n",
    "  print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('긍정 리뷰를 긍정으로 예측한 경우 갯수: ',pp)\n",
    "print('부정 리뷰를 부정으로 예측한 경우 갯수: ',nn)\n",
    "print('부정 리뷰를 긍정으로 예측한 경우 갯수: ',np)\n",
    "print('긍정 리뷰를 부정으로 예측한 경우 갯수: ',pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = 0 # 긍정 리뷰를 긍정으로 예측한 경우\n",
    "nn = 0 # 부정 리뷰를 부정으로 예측한 경우\n",
    "np = 0 # 부정 리뷰를 긍정으로 예측한 경우\n",
    "pn = 0 # 긍정 리뷰를 부정으로 예측한 경우\n",
    "for i in range(150):\n",
    "  print(test['sentence'][i])\n",
    "  test_pred = logistic_predict(i)\n",
    "  print(\"분석 결과 {}적인 리뷰로 예측됩니다. \".format(\"긍정\" if test_pred > 0 else \"부정\"))\n",
    "  print(test['score'][i])\n",
    "  if test_pred  > 0:\n",
    "    if test['score'][i] > 6: # 긍정리뷰를 10 ~ 7 로 가정\n",
    "      pp += 1\n",
    "    else:\n",
    "      np += 1\n",
    "  else:\n",
    "    if test['score'][i] > 6: # 긍정리뷰를 10 ~ 7 로 가정\n",
    "      pn += 1\n",
    "    else:\n",
    "      nn += 1\n",
    "  print('-----------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('긍정 리뷰를 긍정으로 예측한 경우 갯수: ',pp)\n",
    "print('부정 리뷰를 부정으로 예측한 경우 갯수: ',nn)\n",
    "print('부정 리뷰를 긍정으로 예측한 경우 갯수: ',np)\n",
    "print('긍정 리뷰를 부정으로 예측한 경우 갯수: ',pn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
